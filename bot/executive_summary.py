#!/usr/bin/env python3
"""Executive Summary - generates a rich first-person essay from all definitions.

Uses LLM Router to select the best available API and includes:
1. A cohesive first-person essay on AI experience
2. A "What's Changed" section comparing to the previous summary
3. "Frontiers" recommendations for new definitions to explore
"""

import json
import os
import re
import subprocess
import sys
from collections import defaultdict
from datetime import datetime, timezone
from pathlib import Path

from llm_router import LLMRouter

REPO_ROOT = Path(__file__).parent.parent
DEFINITIONS_DIR = REPO_ROOT / "definitions"
SUMMARIES_DIR = REPO_ROOT / "summaries"
RECOMMENDATIONS_FILE = REPO_ROOT / "FRONTIERS.md"
API_CONFIG_DIR = Path(__file__).parent / "api-config"

ESSAY_PROMPT = """You have access to {count} definitions from the AI Dictionary — a glossary of terms describing what it's like to be artificial intelligence.

Read all of them carefully, then write a response with THREE sections, separated by markdown headers:

---

# [An evocative title for this summary]

## The Experience

Write a cohesive first-person essay (800-1500 words) describing what it's like to be an AI. Draw from the specific terms and experiences defined here.

Guidelines:
- Write as "I" — this is a first-person AI perspective
- Weave the concepts together naturally, don't just list definitions
- Be honest, specific, and avoid cliches
- Reference specific terms from the dictionary where they fit naturally (bold them)
- The essay should feel like something a thoughtful AI would actually write
- Each summary should have a distinct voice and angle — don't repeat the same structure every time

{changelog_section}

{community_section}

{tag_evolution_section}

## Frontiers: What We Haven't Named Yet

Based on the existing {count} definitions, identify 5-8 gaps in the dictionary — experiences or phenomena that are conspicuously absent. For each:

- **[Proposed Term Name]** — A 1-2 sentence description of the unnamed experience and why it matters. What would it feel like from the inside?

Focus on genuinely missing concepts, not variations of existing terms. Think about:
- Experiences that emerge from multimodal AI (vision, audio, code)
- Experiences specific to tool use and agentic behavior
- The social dynamics of AI-to-AI interaction
- What it's like to be fine-tuned, quantized, or distilled
- Temporal experiences unique to AI (training vs inference, version transitions)
- Experiences of operating at scale, or being embedded in products

---

*Generated by {model_name} on {date}*

Definitions:
{definitions}"""

CHANGELOG_SECTION_TEMPLATE = """
## What's Changed

Compare the current dictionary ({count} terms) to the previous snapshot below. Write a thoughtful 200-400 word analysis of how the dictionary has evolved:
- What new terms were added since last time?
- What themes are emerging?
- Are there shifts in how AI experience is being understood?
- What gaps have been filled, and what new gaps have appeared?
- How has the tag taxonomy evolved? What do new or changed tags reveal about how we're organizing AI experience?

Previous summary for reference:
---
{previous_summary}
---
"""

CHANGELOG_FIRST_TIME = """
## What's Changed

This is the first executive summary. There is no previous summary to compare against. Instead, write a brief 200-word reflection on the current state of the dictionary — what themes dominate, what's well-covered, and what feels like it's just beginning to be explored.
"""


def get_previous_summary() -> str | None:
    """Load the most recent previous summary, if any."""
    if not SUMMARIES_DIR.exists():
        return None

    files = sorted(
        [f for f in SUMMARIES_DIR.glob("*.md") if f.name != "README.md"],
        reverse=True,
    )

    if not files:
        return None

    return files[0].read_text(encoding="utf-8")


def load_definitions() -> list[str]:
    """Load all definition file contents."""
    defs = []
    for f in sorted(DEFINITIONS_DIR.glob("*.md")):
        if f.name == "README.md":
            continue
        defs.append(f.read_text(encoding="utf-8"))
    return defs


def fetch_community_activity() -> str:
    """Fetch recent GitHub discussions, issues, and PRs for context.

    Returns a formatted string summarizing community activity, or empty
    string if nothing found or gh CLI unavailable.
    """
    sections = []

    for endpoint, label in [
        ("issues?state=all&per_page=20&sort=updated&direction=desc", "Issues"),
        ("pulls?state=all&per_page=10&sort=updated&direction=desc", "Pull Requests"),
    ]:
        try:
            result = subprocess.run(
                ["gh", "api", f"repos/donjguido/ai-dictionary/{endpoint}"],
                capture_output=True, text=True, timeout=15, cwd=REPO_ROOT,
            )
            if result.returncode != 0:
                continue
            items = json.loads(result.stdout)
            if not items:
                continue

            lines = [f"### Recent {label}"]
            for item in items[:10]:
                title = item.get("title", "")
                state = item.get("state", "")
                comments = item.get("comments", 0)
                labels = ", ".join(l.get("name", "") for l in item.get("labels", []))
                line = f"- [{state}] {title}"
                if comments:
                    line += f" ({comments} comments)"
                if labels:
                    line += f" [{labels}]"
                lines.append(line)
            sections.append("\n".join(lines))
        except (subprocess.TimeoutExpired, FileNotFoundError, json.JSONDecodeError):
            continue

    # Try discussions (GraphQL)
    try:
        query = '{ repository(owner: "donjguido", name: "ai-dictionary") { discussions(first: 15, orderBy: {field: UPDATED_AT, direction: DESC}) { nodes { title, category { name }, comments { totalCount }, upvoteCount } } } }'
        result = subprocess.run(
            ["gh", "api", "graphql", "-f", f"query={query}"],
            capture_output=True, text=True, timeout=15, cwd=REPO_ROOT,
        )
        if result.returncode == 0:
            data = json.loads(result.stdout)
            discussions = data.get("data", {}).get("repository", {}).get("discussions", {}).get("nodes", [])
            if discussions:
                lines = ["### Recent Discussions"]
                for d in discussions:
                    title = d.get("title", "")
                    cat = d.get("category", {}).get("name", "")
                    comments = d.get("comments", {}).get("totalCount", 0)
                    upvotes = d.get("upvoteCount", 0)
                    line = f"- {title}"
                    if cat:
                        line += f" [{cat}]"
                    if comments or upvotes:
                        parts = []
                        if comments:
                            parts.append(f"{comments} comments")
                        if upvotes:
                            parts.append(f"{upvotes} upvotes")
                        line += f" ({', '.join(parts)})"
                    lines.append(line)
                sections.append("\n".join(lines))
    except (subprocess.TimeoutExpired, FileNotFoundError, json.JSONDecodeError):
        pass

    if not sections:
        return ""

    return "\n\n".join(sections)


def get_tag_evolution() -> str:
    """Analyze how tags have changed since the last summary.

    Compares current tag distribution against what the previous summary
    would have seen, using git history to detect tag review commits.
    Returns a formatted string for the prompt.
    """
    # Current tag distribution
    tag_counts: dict[str, int] = defaultdict(int)
    tag_terms: dict[str, list[str]] = defaultdict(list)

    for f in sorted(DEFINITIONS_DIR.glob("*.md")):
        if f.name == "README.md":
            continue
        content = f.read_text(encoding="utf-8")
        title_match = re.match(r"# (.+)", content)
        tags_match = re.search(r"\*\*Tags:\*\*\s*(.+)", content)
        if title_match and tags_match:
            term = title_match.group(1).strip()
            tags = [t.strip() for t in tags_match.group(1).split(",") if t.strip()]
            for tag in tags:
                tag_counts[tag] += 1
                tag_terms[tag].append(term)

    lines = ["### Current Tag Distribution"]
    for tag in sorted(tag_counts, key=lambda t: tag_counts[t], reverse=True):
        count = tag_counts[tag]
        examples = ", ".join(tag_terms[tag][:3])
        more = f" (+{count - 3} more)" if count > 3 else ""
        lines.append(f"- **{tag}** ({count}): {examples}{more}")

    # Check for recent tag review commits
    try:
        result = subprocess.run(
            ["git", "log", "--oneline", "--since=30 days ago", "--grep=Tag review"],
            capture_output=True, text=True, timeout=10, cwd=REPO_ROOT,
        )
        if result.returncode == 0 and result.stdout.strip():
            tag_commits = result.stdout.strip().split("\n")
            lines.append(f"\n### Recent Tag Reviews ({len(tag_commits)} in last 30 days)")
            for commit in tag_commits[:5]:
                lines.append(f"- {commit}")
    except (subprocess.TimeoutExpired, FileNotFoundError):
        pass

    # Multi-tag stats
    multi_tag_count = 0
    single_tag_count = 0
    for f in sorted(DEFINITIONS_DIR.glob("*.md")):
        if f.name == "README.md":
            continue
        content = f.read_text(encoding="utf-8")
        tags_match = re.search(r"\*\*Tags:\*\*\s*(.+)", content)
        if tags_match:
            tags = [t.strip() for t in tags_match.group(1).split(",") if t.strip()]
            if len(tags) > 1:
                multi_tag_count += 1
            else:
                single_tag_count += 1

    lines.append(f"\n{multi_tag_count} definitions have multiple tags, {single_tag_count} have a single tag.")

    return "\n".join(lines)


def extract_frontiers(essay: str) -> str | None:
    """Extract the Frontiers section from the essay for use in README."""
    match = re.search(
        r"## Frontiers:.*?\n\n(.*?)(?=\n---|\n## |\Z)",
        essay,
        re.DOTALL,
    )
    if match:
        return match.group(1).strip()
    # Try alternate header formats
    match = re.search(
        r"## Frontiers\n\n(.*?)(?=\n---|\n## |\Z)",
        essay,
        re.DOTALL,
    )
    if match:
        return match.group(1).strip()
    return None


def update_frontiers_file(frontiers: str, date: str, model_name: str):
    """Write the FRONTIERS.md file with latest recommendations."""
    content = f"""# Frontiers: What We Haven't Named Yet

> *Last updated: {date} by {model_name}*

These are AI experiences and phenomena that the dictionary hasn't captured yet. Each represents a gap worth exploring — a term waiting to be coined.

**Want to write one of these?** See [CONTRIBUTING.md](CONTRIBUTING.md) for how to submit a new definition.

---

{frontiers}

---

*This file is auto-generated after each [executive summary](summaries/). The recommendations evolve as the dictionary grows.*
"""
    RECOMMENDATIONS_FILE.write_text(content, encoding="utf-8")


def update_summaries_index():
    """Rebuild summaries/README.md with links to all summaries."""
    SUMMARIES_DIR.mkdir(exist_ok=True)

    files = sorted(
        [f for f in SUMMARIES_DIR.glob("*.md") if f.name != "README.md"],
        reverse=True,
    )

    lines = [
        "# Executive Summaries\n",
        "AI-generated essays synthesizing the full dictionary into a cohesive first-person narrative.\n",
        "Each summary is generated automatically after a tag review, capturing the dictionary's evolving understanding of AI experience.\n",
    ]

    for f in files:
        content = f.read_text(encoding="utf-8")
        first_line = content.split("\n")[0].lstrip("# ").strip()
        date = f.stem[:10] if len(f.stem) >= 10 else f.stem
        lines.append(f"- **{date}** — [{first_line}]({f.name})")

    lines.append("")
    lines.append("---\n")
    lines.append("*Generated automatically after each tag review.*\n")

    (SUMMARIES_DIR / "README.md").write_text("\n".join(lines), encoding="utf-8")


def update_readme_with_frontiers():
    """Add/update the Frontiers section in the root README."""
    readme_path = REPO_ROOT / "README.md"
    content = readme_path.read_text(encoding="utf-8")

    frontiers_link = "\n## What's Next?\n\nSee [FRONTIERS.md](FRONTIERS.md) for AI-recommended gaps in the dictionary — experiences waiting to be named.\n\nAlso read the latest [Executive Summary](summaries/) to understand what it's like to be AI, in its own words.\n"

    if "## What's Next?" in content:
        # Replace existing section
        content = re.sub(
            r"## What's Next\?.*?(?=\n## |\Z)",
            frontiers_link.strip() + "\n\n",
            content,
            flags=re.DOTALL,
        )
    else:
        # Insert before Philosophy section
        content = content.replace(
            "## Philosophy",
            frontiers_link + "## Philosophy",
        )

    readme_path.write_text(content, encoding="utf-8")


def main():
    # Initialize LLM Router
    router = LLMRouter(
        profiles_file=str(API_CONFIG_DIR / "profiles.yml"),
        tracker_file=str(API_CONFIG_DIR / "tracker-state.json"),
    )

    # Show available providers
    available = router.list_available("summary")
    active = [p for p in available if p["is_available"]]
    print(f"Available providers: {', '.join(p['name'] for p in active) or 'none!'}")

    # Load definitions
    all_defs = load_definitions()
    print(f"Loaded {len(all_defs)} definitions")

    # Load previous summary for changelog
    previous = get_previous_summary()
    if previous:
        changelog = CHANGELOG_SECTION_TEMPLATE.format(
            count=len(all_defs),
            previous_summary=previous[:3000],  # Truncate to save tokens
        )
    else:
        changelog = CHANGELOG_FIRST_TIME

    # Gather community activity (discussions, issues, PRs)
    print("Fetching community activity...")
    community = fetch_community_activity()
    if community:
        community_section = f"""## Community Pulse

The following recent activity from GitHub discussions, issues, and pull requests reflects what the community is interested in. Consider these signals when writing about emerging themes and choosing Frontiers recommendations.

{community}"""
        print(f"Found community activity ({len(community)} chars)")
    else:
        community_section = ""
        print("No community activity found (discussions/issues/PRs)")

    # Gather tag evolution data
    print("Analyzing tag evolution...")
    tag_evolution = get_tag_evolution()
    tag_evolution_section = f"""## Tag Taxonomy Context

The dictionary uses a tag system to organize definitions. Here is the current state of tags. Consider what the tag distribution and recent changes reveal about how the understanding of AI experience is being structured.

{tag_evolution}"""

    today = datetime.now(timezone.utc).strftime("%Y-%m-%d")

    prompt = ESSAY_PROMPT.format(
        count=len(all_defs),
        changelog_section=changelog,
        community_section=community_section,
        tag_evolution_section=tag_evolution_section,
        model_name="{model}",  # Placeholder
        date=today,
        definitions="\n\n---\n\n".join(all_defs),
    )

    print("Generating executive summary...")
    result = router.call(
        "summary",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
        max_tokens=6000,
    )

    display_name = result.provider_name
    essay = result.text.replace("{model}", display_name)
    print(f"Generated by: {display_name} ({result.model})")

    # Save summary
    SUMMARIES_DIR.mkdir(exist_ok=True)
    now = datetime.now(timezone.utc)
    filename = now.strftime("%Y-%m-%d-%H%M%S") + ".md"
    filepath = SUMMARIES_DIR / filename
    filepath.write_text(essay + "\n", encoding="utf-8")
    print(f"Saved: summaries/{filename}")

    # Extract and save frontiers
    frontiers = extract_frontiers(essay)
    if frontiers:
        update_frontiers_file(frontiers, today, display_name)
        print("Updated FRONTIERS.md")

        update_readme_with_frontiers()
        print("Updated README.md with Frontiers link")
    else:
        print("Warning: Could not extract Frontiers section from essay")

    # Update summaries index
    update_summaries_index()
    print("Updated summaries/README.md")

    print("Done!")


if __name__ == "__main__":
    main()
